{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.58224471 -1.50960024  0.176527    0.79178155  0.30196811 -0.30161373\n",
      "  -1.33142454  1.03520074]\n",
      " [ 0.3346742  -0.05008156  0.37999317 -0.28788086  1.2722994   1.05035558\n",
      "   0.08524648  0.09474594]\n",
      " [-0.65639618 -0.52645628 -1.34077361  0.22798092 -0.81253418  0.52518873\n",
      "  -1.56236281 -1.79479478]\n",
      " [ 1.04132585  1.64309208  0.36345151 -0.17791464  0.06417598  0.25562796\n",
      "   1.47619578  0.35006814]]\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1196121  -0.93118551 -0.21318391 -0.39354887 -0.218309    0.52355868\n",
      "  -1.10654571  0.4264803 ]\n",
      " [-0.86357507  1.39708436  0.74549203  1.89053096 -0.52545343 -0.66203244\n",
      "   0.10582103  0.23992747]\n",
      " [ 1.24510077  0.95180163 -0.11933885 -0.05873945 -1.04194888 -1.03681915\n",
      "   0.99478999 -0.44955027]\n",
      " [ 0.69717297  1.42916853  0.60631936  0.75197382 -2.11714635 -0.72952369\n",
      "  -1.19973049 -0.89404013]]\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57863886 -1.17212587 -0.90866264 -0.28582865 -2.03056514  0.60944632\n",
      "  -0.08694745  0.24444667]\n",
      " [-1.29808847 -0.5149831  -0.50175086 -0.5585232  -0.22111698 -0.59729146\n",
      "  -0.74315883  0.754882  ]\n",
      " [-0.18621314  1.64957764 -0.13101278  2.1102843   0.52262535  0.69187027\n",
      "  -1.32194976 -0.13532298]\n",
      " [ 0.43067654 -1.28657597  0.87669984  1.22290297 -0.97511077  0.26949972\n",
      "  -0.83446986  0.89761299]]\n"
     ]
    }
   ],
   "source": [
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.93667603,  1.03432348, -5.26624844, -2.30557739],\n",
       "       [ 0.25713742, -1.95210081, -2.03189304, -3.47121365],\n",
       "       [ 2.18058301, -1.25388203, -1.61702548,  2.96463129],\n",
       "       [-3.02639752,  1.36812031,  3.80675355,  0.75445162]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why the denominator sqrt(d_k)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8442261471544072, 0.8421034182769216, 6.540304155258422)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8442261471544072, 0.8421034182769216, 0.8175380194073028)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "- This is to ensure the words dont get context from words generated in the future.\n",
    "- Not required in the encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.03827177,        -inf,        -inf,        -inf],\n",
       "       [ 0.09091181, -0.69017186,        -inf,        -inf],\n",
       "       [ 0.77095252, -0.44331424, -0.57170484,        -inf],\n",
       "       [-1.0699931 ,  0.48370357,  1.34589063,  0.26673893]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.68591362, 0.31408638, 0.        , 0.        ],\n",
       "       [0.64181621, 0.19057298, 0.1676108 , 0.        ],\n",
       "       [0.04822725, 0.22806276, 0.54012895, 0.18358104]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57863886, -1.17212587, -0.90866264, -0.28582865, -2.03056514,\n",
       "         0.60944632, -0.08694745,  0.24444667],\n",
       "       [-0.01081563, -0.96572628, -0.78085719, -0.37147829, -1.46224212,\n",
       "         0.23042642, -0.2930545 ,  0.40476746],\n",
       "       [ 0.09278788, -0.57394422, -0.70077373,  0.06381755, -1.2577909 ,\n",
       "         0.39328984, -0.41900333,  0.27806836],\n",
       "       [-0.28965454,  0.48081682, -0.06807131,  1.22316437, -0.04508388,\n",
       "         0.31634618, -1.04089627,  0.27564233]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57863886, -1.17212587, -0.90866264, -0.28582865, -2.03056514,\n",
       "         0.60944632, -0.08694745,  0.24444667],\n",
       "       [-1.29808847, -0.5149831 , -0.50175086, -0.5585232 , -0.22111698,\n",
       "        -0.59729146, -0.74315883,  0.754882  ],\n",
       "       [-0.18621314,  1.64957764, -0.13101278,  2.1102843 ,  0.52262535,\n",
       "         0.69187027, -1.32194976, -0.13532298],\n",
       "       [ 0.43067654, -1.28657597,  0.87669984,  1.22290297, -0.97511077,\n",
       "         0.26949972, -0.83446986,  0.89761299]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code for Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention\n",
    "\n",
    "def create_causal_mask(L):\n",
    "    mask = np.triu(np.ones((L, L)), k=1)\n",
    "    mask[mask == 1] = -np.inf\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "np.random.seed(42)\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)\n",
    "\n",
    "mask = create_causal_mask(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Encoder Self-Attention ===\n",
      "Q:\n",
      " [[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "   1.57921282  0.76743473]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
      "  -1.72491783 -0.56228753]\n",
      " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
      "   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      "  -0.60170661  1.85227818]]\n",
      "K:\n",
      " [[-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636  -1.95967012\n",
      "  -1.32818605  0.19686124]\n",
      " [ 0.73846658  0.17136828 -0.11564828 -0.3011037  -1.47852199 -0.71984421\n",
      "  -0.46063877  1.05712223]\n",
      " [ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
      "   1.03099952  0.93128012]\n",
      " [-0.83921752 -0.30921238  0.33126343  0.97554513 -0.47917424 -0.18565898\n",
      "  -1.10633497 -1.19620662]]\n",
      "V:\n",
      " [[ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
      "   0.36139561  1.53803657]\n",
      " [-0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
      "   0.09176078 -1.98756891]\n",
      " [-0.21967189  0.35711257  1.47789404 -0.51827022 -0.8084936  -0.50175704\n",
      "   0.91540212  0.32875111]\n",
      " [-0.5297602   0.51326743  0.09707755  0.96864499 -0.70205309 -0.32766215\n",
      "  -0.39210815 -1.46351495]]\n",
      "Output:\n",
      " [[-0.1308104   0.77212573  0.10108921  0.16807328 -0.46588684 -0.43681263\n",
      "   0.46851458 -0.42075407]\n",
      " [ 0.40109276  1.19080398 -0.35037302  0.94668908  0.08274232 -0.53010106\n",
      "   0.17683369  0.41923385]\n",
      " [ 0.17910025  0.98456145 -0.06763014  0.80678092 -0.14453166 -0.49373081\n",
      "   0.15002954  0.10067088]\n",
      " [ 0.01421368  1.14907671 -0.99239485  0.60451701 -0.14600018 -0.40496816\n",
      "   0.24215067 -0.82777073]]\n",
      "Attention Weights:\n",
      " [[0.08431243 0.25513027 0.51521078 0.14534652]\n",
      " [0.64059204 0.1332861  0.01664257 0.2094793 ]\n",
      " [0.47006414 0.08789379 0.11121405 0.33082801]\n",
      " [0.17794451 0.49185018 0.20052305 0.12968226]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Encoder Self-Attention ===\")\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=None)\n",
    "print(\"Q:\\n\", q)\n",
    "print(\"K:\\n\", k)\n",
    "print(\"V:\\n\", v)\n",
    "print(\"Output:\\n\", values)\n",
    "print(\"Attention Weights:\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decoder Self-Attention ===\n",
      "Q:\n",
      " [[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "   1.57921282  0.76743473]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
      "  -1.72491783 -0.56228753]\n",
      " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
      "   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      "  -0.60170661  1.85227818]]\n",
      "K:\n",
      " [[-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636  -1.95967012\n",
      "  -1.32818605  0.19686124]\n",
      " [ 0.73846658  0.17136828 -0.11564828 -0.3011037  -1.47852199 -0.71984421\n",
      "  -0.46063877  1.05712223]\n",
      " [ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
      "   1.03099952  0.93128012]\n",
      " [-0.83921752 -0.30921238  0.33126343  0.97554513 -0.47917424 -0.18565898\n",
      "  -1.10633497 -1.19620662]]\n",
      "V:\n",
      " [[ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
      "   0.36139561  1.53803657]\n",
      " [-0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
      "   0.09176078 -1.98756891]\n",
      " [-0.21967189  0.35711257  1.47789404 -0.51827022 -0.8084936  -0.50175704\n",
      "   0.91540212  0.32875111]\n",
      " [-0.5297602   0.51326743  0.09707755  0.96864499 -0.70205309 -0.32766215\n",
      "  -0.39210815 -1.46351495]]\n",
      "Mask:\n",
      " [[  0. -inf -inf -inf]\n",
      " [  0.   0. -inf -inf]\n",
      " [  0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.]]\n",
      "Output:\n",
      " [[ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
      "   0.36139561  1.53803657]\n",
      " [ 0.66641301  1.39213367 -0.51081002  0.97225045  0.31434319 -0.58550834\n",
      "   0.31495603  0.93081668]\n",
      " [ 0.52954961  1.21756173 -0.14905901  0.7267579   0.1310981  -0.57583252\n",
      "   0.41805381  0.87397953]\n",
      " [ 0.01421368  1.14907671 -0.99239485  0.60451701 -0.14600018 -0.40496816\n",
      "   0.24215067 -0.82777073]]\n",
      "Attention Weights:\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.82776862 0.17223138 0.         0.        ]\n",
      " [0.7024564  0.13134709 0.16619652 0.        ]\n",
      " [0.17794451 0.49185018 0.20052305 0.12968226]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Decoder Self-Attention ===\")\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q:\\n\", q)\n",
    "print(\"K:\\n\", k)\n",
    "print(\"V:\\n\", v)\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Output:\\n\", values)\n",
    "print(\"Attention Weights:\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
